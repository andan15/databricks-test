{"cells":[{"cell_type":"code","source":["%run /Users/antonina.danylenko@nentgroup.com/cf/conf\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run /Users/antonina.danylenko@nentgroup.com/cf/cleaning"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%run /Users/antonina.danylenko@nentgroup.com/cf/als"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import numpy\nimport math\nimport time\nimport threading\nfrom boto.s3.key import Key\nfrom datetime import date, datetime\nfrom numpy.linalg import norm\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.mllib.recommendation import ALS, Rating\nfrom scipy.spatial.distance import chebyshev, cosine\nfrom scipy.stats import entropy\nimport json\n\n\n\"\"\"\nFor a thorough documentation of the functions, refer to the seperate markdown\ndocument \"ensemble.md\".\n\"\"\"\n\ndef get_logger(sc):\n    return sc._jvm.org.apache.log4j.LogManager.getLogger('[RECOMMENDATIONS-EMR][ENSEMBLE]')\n  \ndef read_input_viewing_data(configurations):\n    return sc.textFile(configurations['read']['rawData'])\n\ndef read_program_feed_from_s3(conf):\n    name = conf['read']['programFeed']\n    with open(name, 'r') as f:\n        js = json.load(f)\n    return js  \n\ndef ensemble(configurations):\n    input_viewing_data = read_input_viewing_data(configurations)\n    program_feed = read_program_feed_from_s3(configurations)\n    \n    print 'cleaning starts'\n    preprocessed_datasets = clean_data(configurations, input_viewing_data, program_feed)\n    print 'cleaning finished'\n    \n    print 'create_als_dict starts'\n    children_als = precalculate_als_data(configurations, preprocessed_datasets['children'])\n    regular_als = precalculate_als_data(configurations, preprocessed_datasets['regular'])\n    print 'create_als_dict finished'\n    \n    folders = [\"children\", \"regular\"]\n    i = 0\n    for als_data in [children_als, regular_als]:\n        folder = folders[i]\n        i = i+1\n        log.info('rank_and_save_als_data starts')\n        rank_and_save_als_data(configurations,\n            products_available = als_data['s3']['prods']['available'],\n            features = als_data['ensemble']['als'],\n            suffix = als_data['s3']['saveSuffix'],\n                               folder_name=folder\n        )\n     \n    \n    print 'rank_and_save_als_data finished'\n    \n    \n\n\n\nconfigurations = read_configuration()\nsc.setCheckpointDir('dbfs:/databricks/cf/tmp')\n\nlog = get_logger(sc)\nlog.info('ensemble starts')\nensemble(configurations)\nlog.info('ensemble finished')\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sh\nls /dbfs/mnt/tonja/output"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["file = open(\"/dbfs/databricks/cf/similar-titles/test.txt\", 'w+') \nfile.write(\"Hello!\") \nfile.close() "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sh\nless /dbfs/antonina/output/cf/children/recommendations/part-00000"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"ensemble","notebookId":248},"nbformat":4,"nbformat_minor":0}